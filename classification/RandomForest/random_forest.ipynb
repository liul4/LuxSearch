{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/chris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    " # This file will use the random forest algorithm to classify the text in the dataset to predict the polarity of the text.\n",
    "    # The dataset is a collection of reddit post that is manually labeled as positive or negative.\n",
    "\n",
    "# Import the necessary libraries\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Modelling \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "R_df = pd.read_csv('../Data/Relavance.csv')\n",
    "P_df = pd.read_csv('../Data/Polarity.csv')\n",
    "\n",
    "# # Remove special characters from Title and Data columns such as as ð,Ÿ,˜,â,€,™, and ðŸ˜\n",
    "R_df['Data'] = R_df['Data'].str.replace('ðŸ˜','')\n",
    "R_df['Data'] = R_df['Data'].str.replace('ðŸ‘€','')\n",
    "# Remove ðŸ\n",
    "P_df['Data'] = P_df['Data'].str.replace('ðŸ˜','')\n",
    "P_df['Data'] = P_df['Data'].str.replace('ðŸ‘€','')\n",
    "\n",
    "# # Replace all the NaN values with empty string\n",
    "R_df['Data'] = R_df['Data'].fillna('')\n",
    "R_df['Relavance'] = R_df['Relavance'].fillna('')\n",
    "P_df['Data'] = P_df['Data'].fillna('')    \n",
    "P_df['Polarity'] = P_df['Polarity'].fillna('')\n",
    "\n",
    "# Replace \\n with empty string\n",
    "R_df['Data'] = R_df['Data'].str.replace('\\n','')\n",
    "P_df['Data'] = P_df['Data'].str.replace('\\n','')\n",
    "\n",
    "# Flags to run either the relavance or polarity model\n",
    "relavance = True\n",
    "polarity = True\n",
    "\n",
    "# print(df.head())\n",
    "# print(df.shape)\n",
    "# print(R_df.info())\n",
    "# print(R_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Function for each data\n",
    "def preprocess_bows(df, df_size):\n",
    "    data_soup = BeautifulSoup(df)\n",
    "    data_text = data_soup.get_text()\n",
    "    data_letters_only = re.sub(\"[^a-zA-Z]\", \" \", data_text).lower()\n",
    "    data_words = data_letters_only.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in data_words if not w in stops]\n",
    "\n",
    "    if((i)%500 == 0):\n",
    "        print(\"Cleaned %d %d data (%d %%).\" % (i, df_size, (i/df_size)*100))\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 0 4300 data (0 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2q/bjqpws1153b0_y18lhffmh_m0000gn/T/ipykernel_71366/771688903.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  R_df['Data'][i] = preprocess_bows(R_df['Data'][i], R_df_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 500 4300 data (11 %).\n",
      "Cleaned 1000 4300 data (23 %).\n",
      "Cleaned 1500 4300 data (34 %).\n",
      "Cleaned 2000 4300 data (46 %).\n",
      "Cleaned 2500 4300 data (58 %).\n",
      "Cleaned 3000 4300 data (69 %).\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "R_df_size = R_df['Data'].size\n",
    "P_df_size = P_df['Data'].size\n",
    "\n",
    "if relavance:\n",
    "    for i in range(R_df_size):\n",
    "        R_df['Data'][i] = preprocess_bows(R_df['Data'][i], R_df_size)\n",
    "    print(\"Relavance Data Cleaned\")\n",
    "\n",
    "if polarity:\n",
    "    for i in range(P_df_size):\n",
    "        P_df['Data'][i] = preprocess_bows(P_df['Data'][i], P_df_size)\n",
    "    print(\"Polarity Data Cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity\n",
      "(3010,) (1290,) (3010,) (1290,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare Training Features\n",
    "\n",
    "if relavance:\n",
    "    R_cv = CountVectorizer( analyzer='word', tokenizer=None, preprocessor=None, stop_words=None, max_features=5000)\n",
    "    X_R = R_df['Data']\n",
    "    y_R = R_df['Relavance']\n",
    "\n",
    "    # Split the dataset into training and testing set\n",
    "    X_R_train, X_R_test, y_R_train, y_R_test = train_test_split(X_R, y_R, test_size=0.3, random_state=42)\n",
    "    print(\"Relavance test split done\")\n",
    "    print(X_R_train.shape, X_R_test.shape, y_R_train.shape, y_R_test.shape)\n",
    "\n",
    "if polarity:\n",
    "    P_cv = CountVectorizer( analyzer='word', tokenizer=None, preprocessor=None, stop_words=None, max_features=5000)\n",
    "    # X = P_df.drop('Polarity', axis=1)\n",
    "    X_P = P_df['Data']\n",
    "    y_P = P_df['Polarity']\n",
    "\n",
    "    # Split the dataset into training and testing set\n",
    "    X_P_train, X_P_test, y_P_train, y_P_test = train_test_split(X_P, y_P, test_size=0.3, random_state=42)\n",
    "    print(\"Polarity test split done\")\n",
    "    print(X_P_train.shape, X_P_test.shape, y_P_train.shape, y_P_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train, validation and test data to vectors\n",
    "if relavance:\n",
    "    X_R_train = R_cv.fit_transform(X_R_train)\n",
    "    X_R_test = R_cv.transform(X_R_test)\n",
    "    X_R_train = X_R_train.toarray()\n",
    "    X_R_test = X_R_test.toarray()\n",
    "if polarity:\n",
    "    X_P_train = P_cv.fit_transform(X_P_train)\n",
    "    X_P_test = P_cv.transform(X_P_test)\n",
    "    X_P_train = X_P_train.toarray()\n",
    "    X_P_test = X_P_test.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaml' 'ab' 'abilities' ... 'zipper' 'zverev' 'zverevs']\n",
      "Printing first 10 vocab-dist pairs:\n",
      "5 aaml\n",
      "4 ab\n",
      "5 abilities\n",
      "9 ability\n",
      "51 able\n",
      "7 absolu\n",
      "5 absolueml\n",
      "10 absoluml\n",
      "7 absolute\n",
      "25 absolutely\n"
     ]
    }
   ],
   "source": [
    "if relavance:\n",
    "    vocab_R = R_cv.get_feature_names_out()\n",
    "    print(vocab_R)\n",
    "if polarity:\n",
    "    vocab_P = P_cv.get_feature_names_out()\n",
    "    print(vocab_P)\n",
    "    distribution_P = np.sum(X_P_train, axis=0)\n",
    "    print(\"Printing first 10 vocab-dist pairs:\")\n",
    "    for tag, count in zip(vocab_P[:10], distribution_P[:10]):\n",
    "        print(count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode the labels\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# y_P_train = le.fit_transform(y_P_train)\n",
    "# y_P_test = le.fit_transform(y_P_test)\n",
    "\n",
    "# Fitting and Training the model\n",
    "if relavance:\n",
    "    rf_R = RandomForestClassifier()\n",
    "    rf_R.fit(X_R_train, y_R_train)\n",
    "    print(\"Relavance Model Fitted\")\n",
    "if polarity:\n",
    "    rf_P = RandomForestClassifier()\n",
    "    rf_P.fit(X_P_train, y_P_train)\n",
    "    print(\"Polarity Model Fitted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity Accuracy:  0.6441860465116279\n"
     ]
    }
   ],
   "source": [
    "# Predicting the test set results\n",
    "if relavance:\n",
    "    y_R_pred = rf_R.predict(X_R_test)\n",
    "    print(\"Relavance Accuracy: \", accuracy_score(y_R_test, y_R_pred))\n",
    "if polarity:\n",
    "    y_P_pred = rf_P.predict(X_P_test)\n",
    "    print(\"Polarity Accuracy: \", accuracy_score(y_P_test, y_P_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
